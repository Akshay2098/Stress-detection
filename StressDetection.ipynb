{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6B1PqR29hICn",
        "outputId": "d5500cef-24a2-485d-ea8a-e52278830050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils import face_utils\n",
        "\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
        "\n",
        "# detect the face rectangle \n",
        "def detect(img, cascade = face_cascade , minimumFeatureSize=(20, 20)):\n",
        "    if cascade.empty():\n",
        "        raise (Exception(\"There was a problem loading your Haar Cascade xml file.\"))\n",
        "    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=1, minSize=minimumFeatureSize)\n",
        "    \n",
        "    # if it doesn't return rectangle return array\n",
        "    # with zero lenght\n",
        "    if len(rects) == 0:\n",
        "        return []\n",
        "\n",
        "    #  convert last coord from (width,height) to (maxX, maxY)\n",
        "    rects[:, 2:] += rects[:, :2]\n",
        "\n",
        "    return rects\n",
        "\n",
        "def cropEyes(frame):\n",
        "\t \n",
        "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\t\n",
        "\t# detect the face at grayscale image\n",
        "\tte = detect(gray, minimumFeatureSize=(80, 80))\n",
        "\n",
        "\t# if the face detector doesn't detect face\n",
        "\t# return None, else if detects more than one faces\n",
        "\t# keep the bigger and if it is only one keep one dim\n",
        "\tif len(te) == 0:\n",
        "\t\treturn None\n",
        "\telif len(te) > 1:\n",
        "\t\tface = te[0]\n",
        "\telif len(te) == 1:\n",
        "\t\t[face] = te\n",
        "\n",
        "\t# keep the face region from the whole frame\n",
        "\tface_rect = dlib.rectangle(left = int(face[0]), top = int(face[1]),\n",
        "\t\t\t\t\t\t\t\tright = int(face[2]), bottom = int(face[3]))\n",
        "\t\n",
        "\t# determine the facial landmarks for the face region\n",
        "\tshape = predictor(gray, face_rect)\n",
        "\tshape = face_utils.shape_to_np(shape)\n",
        "\n",
        "\t#  grab the indexes of the facial landmarks for the left and\n",
        "\t#  right eye, respectively\n",
        "\t(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "\t(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
        "\n",
        "\t# extract the left and right eye coordinates\n",
        "\tleftEye = shape[lStart:lEnd]\n",
        "\trightEye = shape[rStart:rEnd]\n",
        "\n",
        "\t# keep the upper and the lower limit of the eye \n",
        "\t# and compute the height \n",
        "\tl_uppery = min(leftEye[1:3,1])\n",
        "\tl_lowy = max(leftEye[4:,1])\n",
        "\tl_dify = abs(l_uppery - l_lowy)\n",
        "\n",
        "\t# compute the width of the eye\n",
        "\tlw = (leftEye[3][0] - leftEye[0][0])\n",
        "\n",
        "\t# we want the image for the cnn to be (26,34)\n",
        "\t# so we add the half of the difference at x and y\n",
        "\t# axis from the width at height respectively left-right\n",
        "\t# and up-down \n",
        "\tminxl = (leftEye[0][0] - ((34-lw)/2))\n",
        "\tmaxxl = (leftEye[3][0] + ((34-lw)/2)) \n",
        "\tminyl = (l_uppery - ((26-l_dify)/2))\n",
        "\tmaxyl = (l_lowy + ((26-l_dify)/2))\n",
        "\t\n",
        "\t# crop the eye rectangle from the frame\n",
        "\tleft_eye_rect = np.rint([minxl, minyl, maxxl, maxyl])\n",
        "\tleft_eye_rect = left_eye_rect.astype(int)\n",
        "\tleft_eye_image = gray[(left_eye_rect[1]):left_eye_rect[3], (left_eye_rect[0]):left_eye_rect[2]]\n",
        "\t\n",
        "\t# same as left eye at right eye\n",
        "\tr_uppery = min(rightEye[1:3,1])\n",
        "\tr_lowy = max(rightEye[4:,1])\n",
        "\tr_dify = abs(r_uppery - r_lowy)\n",
        "\trw = (rightEye[3][0] - rightEye[0][0])\n",
        "\tminxr = (rightEye[0][0]-((34-rw)/2))\n",
        "\tmaxxr = (rightEye[3][0] + ((34-rw)/2))\n",
        "\tminyr = (r_uppery - ((26-r_dify)/2))\n",
        "\tmaxyr = (r_lowy + ((26-r_dify)/2))\n",
        "\tright_eye_rect = np.rint([minxr, minyr, maxxr, maxyr])\n",
        "\tright_eye_rect = right_eye_rect.astype(int)\n",
        "\tright_eye_image = gray[right_eye_rect[1]:right_eye_rect[3], right_eye_rect[0]:right_eye_rect[2]]\n",
        "\n",
        "\t# if it doesn't detect left or right eye return None\n",
        "\tif 0 in left_eye_image.shape or 0 in right_eye_image.shape:\n",
        "\t\treturn None\n",
        "\t# resize for the conv net\n",
        "\tleft_eye_image = cv2.resize(left_eye_image, (34, 26))\n",
        "\tright_eye_image = cv2.resize(right_eye_image, (34, 26))\n",
        "\tright_eye_image = cv2.flip(right_eye_image, 1)\n",
        "\t# return left and right eye\n",
        "\treturn left_eye_image, right_eye_image \n",
        "\n",
        "# make the image to have the same format as at training \n",
        "def cnnPreprocess(img):\n",
        "\timg = img.astype('float32')\n",
        "\timg /= 255\n",
        "\timg = np.expand_dims(img, axis=2)\n",
        "\timg = np.expand_dims(img, axis=0)\n",
        "\treturn img\n",
        "\n",
        "def main():\n",
        "\t# open the camera,load the cnn model \n",
        "\tcamera = cv2.VideoCapture('video.mp4')\n",
        "\tmodel = load_model('blinkModel.hdf5')\n",
        "\t\n",
        "\t# blinks is the number of total blinks ,close_counter\n",
        "\t# the counter for consecutive close predictions\n",
        "\t# and mem_counter the counter of the previous loop \n",
        "\tclose_counter = blinks = mem_counter= 0\n",
        "\tstate = ''\n",
        "\twhile True:\n",
        "\t\t\n",
        "\t\tret, frame = camera.read()\n",
        "\t\t\n",
        "\t\t# detect eyes\n",
        "\t\teyes = cropEyes(frame)\n",
        "\t\tif eyes is None:\n",
        "\t\t\tcontinue\n",
        "\t\telse:\n",
        "\t\t\tleft_eye,right_eye = eyes\n",
        "\t\t\n",
        "\t\t# average the predictions of the two eyes \n",
        "\t\tprediction = (model.predict(cnnPreprocess(left_eye)) + model.predict(cnnPreprocess(right_eye)))/2.0\n",
        "\t\t\t\n",
        "\t\t# blinks\n",
        "\t\t# if the eyes are open reset the counter for close eyes\n",
        "\t\tif prediction > 0.5 :\n",
        "\t\t\tstate = 'open'\n",
        "\t\t\tclose_counter = 0\n",
        "\t\telse:\n",
        "\t\t\tstate = 'close'\n",
        "\t\t\tclose_counter += 1\n",
        "\t\t\n",
        "\t\t# if the eyes are open and previousle were closed\n",
        "\t\t# for sufficient number of frames then increcement \n",
        "\t\t# the total blinks\n",
        "\t\tif state == 'open' and mem_counter > 1:\n",
        "\t\t\tblinks += 1\n",
        "\t\t# keep the counter for the next loop \n",
        "\t\tmem_counter = close_counter \n",
        "\n",
        "\t\t# draw the total number of blinks on the frame along with\n",
        "\t\t# the state for the frame\n",
        "\t\tcv2.putText(frame, \"Blinks: {}\".format(blinks), (10, 30),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\t\tcv2.putText(frame, \"State: {}\".format(state), (300, 30),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\t\t\n",
        "\t\t# show the frame\n",
        "# \t\tcv2.imshow('blinks counter', frame)\n",
        "\t\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t\t# if the `q` key was pressed, break from the loop\n",
        "\t\tif key == ord('q'):\n",
        "\t\t\tbreak\n",
        "\t# do a little clean up\n",
        "\tcv2.destroyAllWindows()\n",
        "\tdel(camera)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a0a00ea8e848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mface_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mface_cascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haarcascade_frontalface_alt.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vtuY4zHklMif"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#first install mtcnn using  ->      !pip install mtcnn\n",
        "#Then download and upload “shape_predictor_68_face_landmarks.dat”  file\n",
        "#upload the attached video also\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.image as mpimg\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from imutils import face_utils\n",
        "import imutils\n",
        "import dlib\n",
        "\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "                A = distance.euclidean(eye[1], eye[5])\n",
        "                B = distance.euclidean(eye[2], eye[4])\n",
        "                C = distance.euclidean(eye[0], eye[3])\n",
        "                ear = (A + B) / (2.0 * C)\n",
        "                return ear\n",
        "\n",
        "def detect_face(original_frame):\n",
        "  \n",
        "    detector = MTCNN()\n",
        "    original_image = cv2.imread(original_frame)\n",
        "    resultant_json_object = detector.detect_faces(original_image)\n",
        "    print(resultant_json_object)\n",
        "    duplicate_image=original_image.copy()\n",
        "    \n",
        "    index=len(resultant_json_object)\n",
        "    print(index)                  #It'll display count of total face detected. i.e total number of bounding_boxes found\n",
        "    \n",
        "    if(index==0):\n",
        "      print(\"No face detected\")\n",
        "      return ;\n",
        "    \n",
        "\n",
        "    for i in range(0,index):\n",
        "      bounding_box = resultant_json_object[i]['box']\n",
        "#       keypoints = resultant_json_object[i]['keypoints']\n",
        "      confidence=resultant_json_object[i]['confidence']   #accuracy of detected face\n",
        "      cv2.rectangle(original_image,(bounding_box[0],bounding_box[1]),(bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),(0,255,0),3)\n",
        "#       cv2.circle(original_image,(keypoints['left_eye']), 2, (0,155,255), 2)\n",
        "#       cv2.circle(original_image,(keypoints['right_eye']), 2, (0,155,255), 2)\n",
        "#       cv2.circle(original_image,(keypoints['nose']), 2, (0,155,255), 2)\n",
        "#       cv2.circle(original_image,(keypoints['mouth_left']), 2, (0,155,255), 2)\n",
        "#       cv2.circle(original_image,(keypoints['mouth_right']), 2, (0,155,255), 2)\n",
        "\n",
        "      font = cv2.FONT_HERSHEY_DUPLEX \n",
        "      cv2.putText(original_image, str(confidence)[0:7], (bounding_box[0],bounding_box[1]), font,0.75, (0, 0,255),2)\n",
        "      cv2.imwrite(\"Face_detected_image.jpg\", original_image)\n",
        "\n",
        "    pupil_detect(\"Face_detected_image.jpg\",duplicate_image)\n",
        "\n",
        "  \n",
        "def pupil_detect(Face_detected_image,duplicate_image):\n",
        "  \n",
        "  Face_detected_frame=cv2.imread(Face_detected_image)\n",
        "  thresh = 0.25                                    #if ear is less than threshold value then eyes are considered as closed\n",
        "#   frame_check = 20\n",
        "  detect = dlib.get_frontal_face_detector()        #return object for face detection          \n",
        "  predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  # print(predict)\n",
        "  (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "  (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]  \n",
        "\n",
        "\n",
        "  # frame = imutils.resize(frame, width=650)\n",
        "  gray_image = cv2.cvtColor(Face_detected_frame, cv2.COLOR_BGR2GRAY)\n",
        "  subjects = detect(gray_image, 0)\n",
        "  print(subjects)\n",
        "  if(len(subjects)==0):\n",
        "    print(\"Eyes not detected\")\n",
        "    return;\n",
        "  f, ax1=plt.subplots(2,1)\n",
        "  for subject in subjects:\n",
        "    shape = predict(gray, subject)\n",
        "    shape = face_utils.shape_to_np(shape)          #converting to NumPy Array\n",
        "    leftEye = shape[lStart:lEnd]\n",
        "    rightEye = shape[rStart:rEnd]\n",
        "    print(\"leftEye co-ordinates \",leftEye)\n",
        "    print(\"rightEye co-ordinates\",rightEye)\n",
        "    \n",
        "    leftEyeXMid=(leftEye[1][0]+leftEye[5][0]+leftEye[2][0]+leftEye[4][0])//4\n",
        "    leftEyeYMid=(leftEye[1][1]+leftEye[5][1]+leftEye[2][1]+leftEye[4][1])//4\n",
        "\n",
        "    rightEyeXMid=(rightEye[1][0]+rightEye[5][0]+rightEye[2][0]+rightEye[4][0])//4\n",
        "    rightEyeYMid=(rightEye[1][1]+rightEye[5][1]+rightEye[2][1]+rightEye[4][1])//4\n",
        "\n",
        "  #   print(leftEye[1][0],leftEye[1][1])\n",
        "  #   print(leftEye[5][0],leftEye[5][1])\n",
        "  #   print(leftEye[2][0],leftEye[2][1])\n",
        "  #   print(leftEye[4][0],leftEye[4][1])\n",
        "  #   print(leftEyeXMid)\n",
        "  #   print(leftEyeYMid)\n",
        "  \n",
        "    leftEAR = eye_aspect_ratio(leftEye)\n",
        "    rightEAR = eye_aspect_ratio(rightEye)\n",
        "    ear = (leftEAR + rightEAR) / 2.0\n",
        "    \n",
        "  #   print(ear)\n",
        "  #   print(\"left ear\", leftEAR)\n",
        "  #   print(\"right ear\", rightEAR)\n",
        "  #   print(leftEye)\n",
        "    leftEyeHull = cv2.convexHull(leftEye)\n",
        "    rightEyeHull = cv2.convexHull(rightEye)\n",
        "    \n",
        "    cv2.drawContours(Face_detected_frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "    cv2.drawContours(Face_detected_frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\n",
        "\n",
        "    if ear < thresh:                          #To check whether eyes are closed or not\n",
        "      \n",
        "      cv2.putText(Face_detected_frame, \"****************Eyes Closed!****************\", (10, 30),\n",
        "          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      cv2.circle(Face_detected_frame,(leftEyeXMid,leftEyeYMid), 1, (0,155,255), 1)\n",
        "      cv2.circle(Face_detected_frame,(rightEyeXMid,rightEyeYMid), 1, (0,155,255), 1)\n",
        "  # cv2.imshow(\"Frame\", frame)\n",
        "  \n",
        "  cv2.imwrite(\"Pupil_detected_image.jpg\",Face_detected_frame)\n",
        "  img_pupil=mpimg.imread('Pupil_detected_image.jpg')\n",
        "  ax1[0].imshow(duplicate_image)\n",
        "  ax1[1].imshow(img_pupil)\n",
        "  \n",
        "\n",
        "vidcap = cv2.VideoCapture('Video.mp4')\n",
        "count = 0\n",
        "success = True\n",
        "fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "while success:\n",
        "    success,image = vidcap.read()\n",
        "    count += 1\n",
        "#     print('read a new frame:',success)\n",
        "    if(count==600):\n",
        "#       cv2.imwrite(\"frame%d.jpg\" % count, image)\n",
        "      cv2.imwrite(\"original_frame.jpg\", image)\n",
        "      detect_face(\"original_frame.jpg\")\n",
        "        \n",
        "      break;\n",
        "    \n",
        "#     if count%(10*fps) == 0 :\n",
        "#          cv2.imwrite('frame%d.jpg'%count,image)\n",
        "#          print('successfully written 10th frame')\n",
        "#     count+=1\n",
        "#     cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
        "#     cv2.waitKey(2)\n",
        "    \n",
        "  \n",
        "print(\"Successfully Processed!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKqj8KJU5LBW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}